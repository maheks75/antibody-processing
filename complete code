#STEP1
from Bio import SeqIO
input_fastq_file = "A.fastq"
output_file_5g_with_g = "MERGEDAL_t_With5G_2.fastq"
output_file_5g_without_g = "MERGEDAL_t_Without5G_2.fastq"
output_file_4g_with_g = "MERGEDAL_t_With4G_2.fastq"
output_file_4g_without_g = "MERGEDAL_t_Without4G_2.fastq"
with open(output_file_5g_with_g, 'w') as output_handle_with_5g, open(output_file_5g_without_g, 'w') as output_handle_without_5g:
    for i, record in enumerate(SeqIO.parse(input_fastq_file, "fastq")):
        sequence_data = str(record.seq)[:25]
        print(sequence_data)

        if 'GGGGG' in sequence_data:
            output_handle_with_5g.write(f"{sequence_data}\n")
        
with open(output_file_4g_with_g, 'w') as output_handle_with_4g, open(output_file_4g_without_g, 'w') as output_handle_without_4g:
    for i, record in enumerate(SeqIO.parse(input_fastq_file, "fastq")):
        sequence_data = str(record.seq)[:25]
        print(sequence_data)

        if 'GGGG' in sequence_data:
            output_handle_with_4g.write(f"{sequence_data}\n")
        
#STEP2
#final script to extract umi from a file with ggggg
from Bio import SeqIO

input_5g_txt_file = "MERGEDAL_t_With5G_2.fastq"
input_4g_txt_file = "MERGEDAL_t_With4G_2.fastq"
target_sequence_4g = 'GGGG'
target_sequence_5g = 'GGGGG'
output_file_4G = "MERGEDAL_t_With4G_Extracted.txt"
output_file_5G = "MERGEDAL_t_With5G_Extracted.txt"
sequences_written = 0

with open(input_5g_txt_file, "r") as file, open(output_file_5G, 'w') as output_handle:
    strings = [line.strip() for line in file]
    for string in strings:
        if target_sequence_5g in string[:18]:
            extracted_sequence = string[:12]
            output_handle.write(extracted_sequence + "\n")
            sequences_written += 1
            #print("Extracted Sequence:", extracted_sequence)
        
with open(input_4g_txt_file, "r") as file, open(output_file_4G, 'w') as output_handle:
    strings = [line.strip() for line in file]
    for string in strings:
        if target_sequence_4g in string[:18]:
            extracted_sequence = string[:12]
            output_handle.write(extracted_sequence + "\n")
            sequences_written += 1
            #print("Extracted Sequence:", extracted_sequence)         

#print(f"Number of sequences written to {output_file}: {sequences_written}")
#STEP3
filename_5G = "MERGEDAL_t_With5G_Extracted.txt"
filename_4G = "MERGEDAL_t_With4G_Extracted.txt"

# Read the strings from the file
with open(filename_5G, 'r') as file_5G:
    strings_5G = [line.strip() for line in file_5G]

string_count_5G = {}
for string_5G in strings_5G:
    if string_5G in string_count_5G:
        string_count_5G[string_5G] += 1
    else:
        string_count_5G[string_5G] = 1

repeating_strings_count_5G = 0
non_repeating_strings_count_5G = 0

repeating_filename_5G = "repeating_5g.txt"
non_repeating_filename_5G = "nonrepeating_5g.txt"

repeating_strings_5G = []
non_repeating_strings_5G = []

# Separate repeating and non-repeating strings
for string_5G, count_5G in string_count_5G.items():
    if count_5G > 1:
        repeating_strings_5G.append((string_5G, count_5G))
        repeating_strings_count_5G += count_5G
repeating_strings_5G.sort(key=lambda x: x[1], reverse=True)


# Write repeating strings to file with count
with open(repeating_filename_5G, 'w') as file_5G:
    for string_5G, count_5G in repeating_strings_5G:
        file_5G.write(f"{string_5G}: {count_5G}\n")
with open(filename_4G, 'r') as file:
    strings_4G = [line.strip() for line in file]

string_count_4G = {}
for string_4G in strings_4G:
    if string_4G in string_count_4G:
        string_count_4G[string_4G] += 1
    else:
        string_count_4G[string_4G] = 1

repeating_strings_count_4G = 0
non_repeating_strings_count_4G = 0

repeating_filename_4G = "repeating_4g.txt"
non_repeating_filename_4G = "nonrepeating_4g.txt"

repeating_strings_4G = []
non_repeating_strings_4G = []

# Separate repeating and non-repeating strings
for string_4G, count_4G in string_count_4G.items():
    if count_4G > 1:
        repeating_strings_4G.append((string_4G, count_4G))
        repeating_strings_count_4G += count_4G
repeating_strings_4G.sort(key=lambda x: x[1], reverse=True)


# Write repeating strings to file with count
with open(repeating_filename_4G, 'w') as file:
    for string_4G, count_4G in repeating_strings_4G:
        file.write(f"{string_4G}: {count_4G}\n")        
#STEP4
#final script to separate multiple repeating and strings that are repeating only twice from the total set of repeating strings
filename_5G = "repeating_5g.txt"
filename_4G = "repeating_4g.txt"

# Read the strings from the file
with open(filename_5G, 'r') as file_5G:
    count_5G = 0
    twostringlist_5G = []
    multiplestring_5G = []
    strings_5G = [line.strip() for line in file_5G]
    for string_5G in strings_5G:
        parts_5G = string_5G.split(':')
        if int(parts_5G[1]) < 50:
            twostringlist_5G.append(parts_5G[0])            
        else:
            multiplestring_5G.append(parts_5G[0])
            
        
with open('multiple_strings_5g.txt', 'w') as file_5G:
                for i, stringgg_5G in enumerate(multiplestring_5G):
                    print(stringgg_5G)
                    file_5G.write(f"{stringgg_5G} \n")         
with open(filename_4G, 'r') as file_4G:
    count_4G = 0
    twostringlist_4G = []
    multiplestring_4G = []
    strings_4G = [line.strip() for line in file_4G]
    for string_4G in strings_4G:
        parts_4G = string_4G.split(':')
        if int(parts_4G[1]) < 40:
            twostringlist_4G.append(parts_4G[0])            
        else:
            multiplestring_4G.append(parts_4G[0])
            
        
with open('multiple_strings_4g.txt', 'w') as file_4G:
                for i, stringgg_4G in enumerate(multiplestring_4G):
                    print(stringgg_4G)
                    file_4G.write(f"{stringgg_4G} \n")         
                        

#STEP5

def read_text_file(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()
        # Remove leading and trailing whitespace, and split into a list of strings
        string_list = [line.strip() for line in lines if isinstance(line, str)]
    return string_list

# File paths for the two text files
file1_path = 'A_multiplestrings_5g.txt'
file2_path = 'A_multiplestrings_4g.txt'
# Read and process the text files
file1_strings = read_text_file(file1_path)
file2_strings = read_text_file(file2_path)

# Find the common strings
common_strings = set(file1_strings).intersection(file2_strings)
unique_string = []
# Print the common strings
#print("Common Strings:", common_strings)

# Print the strings that are not common in file1
print("Strings in file1 but not in file2:")
for string in file1_strings:
    if isinstance(string, str) and string not in common_strings:
        print(string)

# Print the strings that are not common in file2
print("Strings in file2 but not in file1:")
for string in file2_strings:
    if isinstance(string, str) and string not in common_strings:
        print(string)
        unique_string.append(string)
    with open('4g_final.txt', 'w') as file:
        for strg in unique_string:
            file.write(strg+'\n')

#STEP6- SEE CONSENSUS CALLING FOR UMI(TYPE1 AND 2)

#STEP 7
from Bio import SeqIO

input_fastq_file_5g = "5g_IL8_T_con.txt"
input_fastq_file_4g = "4g_IL8_T_con.txt"
output_file_full = "merged.txt"
sequence_list_file = "sequence_list.txt"

sequence_full_list = []
sequence_list_only = []

with open(output_file_full, 'w') as file:
    for i, record in enumerate(SeqIO.parse(input_fastq_file_5g, "fasta")):
        header_parts = record.description.split(":")
        header = f">{i+1} {header_parts[2].strip()}"
        sequence_data_5g = str(record.seq)
        print(sequence_data_5g)
        sequence_full_list.append(f"{header}\n{sequence_data_5g}")
        sequence_list_only.append(header_parts[2].strip())

    for i, record in enumerate(SeqIO.parse(input_fastq_file_4g, "fasta")):
        header_parts = record.description.split(":")
        header = f">{i+1} {header_parts[2].strip()}"
        sequence_data_4g = str(record.seq)
        print(sequence_data_4g)
        sequence_full_list.append(f"{header}\n{sequence_data_4g}")
        sequence_list_only.append(header_parts[2].strip())

    for seq_full in sequence_full_list:
        file.write(seq_full + '\n')

# Save the sequence_list_only to a file
with open(sequence_list_file, 'w') as seq_file:
    for seq_part in sequence_list_only:
        seq_file.write(seq_part + '\n')

print("List of sequence headers only:", sequence_list_only)
#STEP8
import sys
from Bio import SeqIO
import subprocess
input_fasta = 'merged.txt'
output_fasta = 'igblast_output.tsv'
igblast_cmd = [
     'igblastn',
     '-query', input_fasta,
     '-out', output_fasta,
     '-germline_db_V', 'AlpacaV',
     '-germline_db_J', 'AlpacaJ',
     '-germline_db_D', 'AlpacaD',
     '-auxiliary_data','camelid_gl.aux',
     '-num_threads', '32',
     '-outfmt', '19',  
]
try:
    subprocess.run(igblast_cmd, check=True)
    print("IgBLAST analysis complete. Output saved to:", output_fasta)
except subprocess.CalledProcessError as e:  
    print("Error running IgBLAST:", e)

#STEP9
import pandas as pd
import sys
# Load the TSV file into a DataFrame
tsv_file_path = 'igblast_output.tsv'
df = pd.read_csv(tsv_file_path, sep='\t')

# Load the text file with strings
strings_file_path = "sequence_list.txt"
with open(strings_file_path, 'r') as strings_file:
    umi_strings = [line.strip() for line in strings_file]

# Create a new "umi" column and add strings
df.insert(1, 'umi', umi_strings[:len(df)])

# Keep columns 0 to 54
df = df.iloc[:, :55]

# EXAMINE DOUBLE HASTAGGED STEPS BELOW IF THE ARE NEEDED OR NOT
##final_output_path = 'output.tsv'
##df.to_csv(final_output_path, sep='\t', index=False)

##tsv_file_path = 'output.tsv'
# Load the CSV file into a DataFrame
##df = pd.read_csv(tsv_file_path, sep='\t')

# Select the columns of interest
# Specify the column numbers you want to select
selected_columns = [0,1,2,3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]

selected_df = df.iloc[:, selected_columns]
#print(selected_df)
# Group by the "sequence_aa" column and aggregate other columns
aggregation_dict = selected_df.groupby('sequence_aa').agg({
                   selected_df.columns[0]: 'sum',
                   selected_df.columns[1]: 'unique',
                   selected_df.columns[4]: 'first',
                   selected_df.columns[5]: 'first',
                   selected_df.columns[6]: 'first',
                   selected_df.columns[7]: 'first',
                   selected_df.columns[8]: 'first',
                   selected_df.columns[9]: 'first',
                   selected_df.columns[10]: 'first',
                   selected_df.columns[11]: 'first',
                   selected_df.columns[12]: 'first',
                   selected_df.columns[13]: 'first',
                   selected_df.columns[14]: 'first',
                   selected_df.columns[15]: 'first',
                   selected_df.columns[16]: 'first',
                   selected_df.columns[17]: 'first',
                   selected_df.columns[18]: 'first',
                   selected_df.columns[19]: 'first',
                   selected_df.columns[20]: 'first',
                   selected_df.columns[21]: 'first',
                   selected_df.columns[22]: 'first',
                   selected_df.columns[23]: 'first',
                   selected_df.columns[24]: 'first',
                   selected_df.columns[25]: 'first',
                   selected_df.columns[26]: 'first',
                   selected_df.columns[27]: 'first',
                   selected_df.columns[28]: 'first',
                   selected_df.columns[29]: 'first',
                   selected_df.columns[30]: 'first',
                   selected_df.columns[31]: 'first',
                   selected_df.columns[32]: 'first',
                   selected_df.columns[33]: 'first',
                   selected_df.columns[34]: 'first',
                   selected_df.columns[35]: 'first',
                   selected_df.columns[36]: 'first',
                   selected_df.columns[37]: 'first',
                   selected_df.columns[38]: 'first',
                   selected_df.columns[39]: 'first',
                   selected_df.columns[40]: 'first',
                   selected_df.columns[41]: 'first',
                   selected_df.columns[42]: 'first',
                   selected_df.columns[43]: 'first',
                   selected_df.columns[44]: 'first',
                   selected_df.columns[45]: 'first',
                   selected_df.columns[46]: 'first',
                   selected_df.columns[47]: 'first',
                   selected_df.columns[48]: 'first',
                   selected_df.columns[49]: 'first',
                   selected_df.columns[50]: 'first',
                   selected_df.columns[51]: 'first',
                   selected_df.columns[52]: 'first',
                   selected_df.columns[53]: 'first',
                   selected_df.columns[54]: 'first'}
).reset_index()

aggregation_dict.to_csv('igblast_output_file.tsv', sep='\t', index=False)
# Create a DataFrame with your UMI data
df = pd.read_csv('igblast_output_file.tsv', sep='\t')
#print(df.columns)
# Split the UMI strings by space and count the number of strings
df['Number_of_Strings'] = df['umi'].apply(lambda x: len(x.split()))
df.to_csv('Final_processed_igblast_file.tsv', sep='\t', index = False)
