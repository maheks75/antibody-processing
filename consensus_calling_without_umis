#finl_multiprocessing script
from itertools import zip_longest
import os
import tempfile
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor
import shutil
from multiprocessing import Pool, Manager
def grouper(n, iterable, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    args = [iter(iterable)] * n
    return zip_longest(fillvalue=fillvalue, *args)

def calculate_con_total(segments, num_seq):  
    print(len(segments))
    #print(num_seq)
    con_total = []
    #print(passed_segment)
    for p in segments:
        #print([p])
        consensus_sequence = ""
        BASES_CALCULATE = {"A": 0, "C": 0, "G": 0, "T": 0, "N": 0}
        bases = ["A", "C", "G", "T", "N"]
        for k in range(len(p)):
            for base in bases:
                BASES_CALCULATE[base] = 0
            for j in range(len(segments)):
                if len(segments[j]) > k:
                    base = segments[j][k]
                    BASES_CALCULATE[base] += 1
            most_common_base = max(BASES_CALCULATE, key=BASES_CALCULATE.get)
            consensus_sequence += most_common_base
    #print([consensus_sequence])
    con_total.append(consensus_sequence)
    #print(con_total)
    return con_total

def process_file(folder_name, sequence_with_max_count, n):
    passed = []
    failed = []

    import os
    passed = []
    for filename in os.listdir(folder_name):
        if filename.endswith(".txt"):
            filepath = os.path.join(folder_name, filename)
            with open(filepath, 'r') as file_2:
                sequence_file2 = [line.strip() for line in file_2]
                for line in sequence_file2:
                    seq, count = line.split(':')
                    mismatch_count = 0
                    if len(seq) == len(sequence_with_max_count):
                        mismatch_count = sum(1 for i in range(len(seq)) if seq[i] != sequence_with_max_count[i])
                        if mismatch_count <= n:
                            passed.append(line)
                        else:
                            failed.append(line)
                    else:
                        failed.append(line)
    result_filename = f'{max_key}.txt'
    result_filepath = os.path.join(folder_name, result_filename)
    with open(result_filepath, 'w') as file_2:
        for p in passed:
            file_2.write(f'{p}\n')

    import concurrent.futures
    l = 40
    final_passed = []
    for p in passed:    
        seq_p = p.split(':')[0]
        final_passed.append(seq_p)
        #print(final_passed)
    def create_seg(final_passed, n):
        num_seq = len(final_passed)
        #print(num_seq)
        max_len = max(len(seq) for seq in final_passed)
        m = max_len // l
        segments = []
        count_seg =0
        for i in range(0, max_len, m):
            segment = [seq[i:i + m] for seq in final_passed]
            segments.append(segment)
            count_seg += 1
    
        return segments, num_seq

    

    def part_2(segments, num_seq):
        import concurrent.futures
        num_cores = 48 # You can adjust this based on your machine's capabilities
    
        with Manager() as manager:
            num_seq = manager.Value('k', 1)  # Create a shared integer variable
        
            with Pool(processes=num_cores) as pool:
                 con_totals = pool.starmap(calculate_con_total, [(segments, num_seq) for segments in segments])


        final_con = ""
        for con_total in con_totals:
            for c in con_total:
                final_con += c
        #print(final_con)
        return final_con
    
    segments, num_seq = create_seg(final_passed, n)
    num_seq = create_seg(final_passed, n)
    result_0 = part_2(segments, num_seq)
    #print(result_0)
    header = f'N_{n}:SeqCount_{len(passed)}:MaxKey_{max_key}'
    
    return f'>{header}\n{result_0}\n', failed

with open('sorted_test.txt', 'r') as file:
    strings = [line.strip() for line in file]

remain_strings = strings.copy()

total_files = 40
lines_per_file = len(remain_strings) // total_files + 1
file_counter = 1
with open('con_file', 'a') as file:
    while remain_strings:
        count_passed = 0
        count_1 = 0
        failed = []
        sequence = [line.split(':')[0] for line in remain_strings]
        count = [int(line.split(':')[1].strip()) for line in remain_strings]

        max_key = max(count)
        if max_key == 20:
            print('max_key less than 20')
            break

        n = 3 if max_key in range(2, 400) else 4 if max_key in range(410, 1000) else 5 if max_key > 1000 else 10
        sequence_with_max_count = sequence[count.index(max(count))]

        lines_per_file = len(remain_strings) // total_files + 1
        folder_name = f"{max_key}_{len(remain_strings)}_files"
        os.makedirs(folder_name, exist_ok=True)
        for file_counter, lines_chunk in enumerate(grouper(lines_per_file, remain_strings), start=1):
            lines_chunk = [line for line in lines_chunk if line is not None]
            lines_to_write = len(lines_chunk)
           # print(lines_to_write)
            with tempfile.NamedTemporaryFile('w', delete=False) as fout:
                for s in lines_chunk:
                    fout.write(f'{s}\n')
        
            new_filename = os.path.join(folder_name, f'small_file_{len(lines_chunk)}_{file_counter}.txt')
            shutil.move(fout.name, new_filename)
        with ThreadPoolExecutor(max_workers=48) as executor:
            results = list(executor.map(
                process_file,
                [folder_name],
                [sequence_with_max_count],
                [n]
            ))

        for result, remaining_strings in results:   
            file.write(f'{result}')
            failed.extend(remaining_strings)

        remain_strings = failed.copy()
        print('BREAK')
